{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from fastai import Flatten\n",
    "import torchvision.models as models\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import label_ranking_loss, coverage_error, label_ranking_average_precision_score\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        for i in ['car', 'aeroplane', 'pottedplant', 'tvmonitor', 'cat', 'bus', 'bird', 'cow', 'dog',\\\n",
    "                'sofa', 'sheep', 'chair', 'bottle', 'person', 'bicycle', 'boat', 'motorbike', 'horse',\\\n",
    "                'diningtable', 'train']:\n",
    "            df[i] = df[i].map({1:1, 0:1, -1:0})\n",
    "        self.features = df.values[:, -2048:].astype(np.float32)\n",
    "        self.labels = df.values[:,-2048-20:-2048].astype(np.int16)\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def preprocess(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    return 1./(1+np.exp(-x))\n",
    "def evaluate(y, y_, pivot=0.5):\n",
    "    true_labels = y\n",
    "    predict_probs = sigmoid(y_)\n",
    "    predict_labels = (predict_probs>pivot).astype(np.int)\n",
    "    #print(\"-----------------example based metrics--------------------------\")\n",
    "    #hammingLoss = hamming_loss(true_labels, predict_labels)\n",
    "    accuracy = jaccard_similarity_score(true_labels, predict_labels)\n",
    "    #subset_accuracy = accuracy_score(true_labels, predict_labels)\n",
    "    #precision = precision_score(true_labels, predict_labels, average=\"samples\")\n",
    "    #recall = recall_score(true_labels, predict_labels, average=\"samples\")\n",
    "    f1Score = f1_score(true_labels, predict_labels, average=\"samples\")\n",
    "\n",
    "    # label based metrics\n",
    "    #print(\"------------------label based metrics---------------------------\")\n",
    "    #micro_precision = precision_score(true_labels, predict_labels, average=\"micro\")\n",
    "    #micro_recall = recall_score(true_labels, predict_labels, average=\"micro\")\n",
    "    micro_F1_score = f1_score(true_labels, predict_labels, average=\"micro\")\n",
    "    #macro_precision = precision_score(true_labels, predict_labels, average=\"macro\")\n",
    "    #macro_recall = recall_score(true_labels, predict_labels, average=\"macro\")\n",
    "    macro_F1_score = f1_score(true_labels, predict_labels, average=\"macro\")\n",
    "\n",
    "    # rank metrics\n",
    "    #print(\"--------------------rank metrics--------------------------------\")\n",
    "    #ranking_loss = label_ranking_loss(true_labels, predict_probs)\n",
    "    #one_error = None\n",
    "    #coverage = coverage_error(true_labels, predict_probs)\n",
    "    MAP = average_precision_score(y_true=true_labels, y_score=predict_probs, average='macro')\n",
    "    return accuracy, f1Score, micro_F1_score, macro_F1_score, MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(2048, 128),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.BatchNorm1d(num_features=128),])\n",
    "        self.fc = nn.Linear(128, 20)\n",
    "    def forward(self, x):\n",
    "        h = self.mlp(x)\n",
    "        y = self.fc(h)\n",
    "        return y\n",
    "\n",
    "from fastai.callback import SmoothenValue\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(*[\n",
    "            nn.Linear(20, 1)\n",
    "        ])\n",
    "    def forward(self, h):\n",
    "        return self.fc(h)\n",
    "\n",
    "class MyGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyGAN, self).__init__()\n",
    "        self.predictor = Predictor()\n",
    "        self.discriminator = Discriminator()\n",
    "    def forward(self, x, y):\n",
    "        y_ = self.predictor(x)\n",
    "        false_logit = self.discriminator((y_>0).float())\n",
    "        true_logit = self.discriminator(y)\n",
    "        return y_, false_logit, true_logit\n",
    "class MyLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(MyLoss, self).__init__()\n",
    "        self.loss1 = nn.BCEWithLogitsLoss() # for expression\n",
    "        self.loss2 = nn.BCEWithLogitsLoss() # for discriminator\n",
    "        self.alpha = alpha\n",
    "    def forward(self, y_, y, false_logit):\n",
    "        l1 = self.loss1(input=y_, target=y)\n",
    "        l2 = self.loss2(input=false_logit, target=torch.ones_like(false_logit))\n",
    "        return (1-self.alpha)*l1 + self.alpha*l2\n",
    "class MyDiscriminatorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDiscriminatorLoss, self).__init__()\n",
    "        self.loss1 = nn.BCEWithLogitsLoss()\n",
    "    def forward(self, false_logit, true_logit):\n",
    "        l1 = self.loss1(input=false_logit, target=torch.zeros_like(false_logit)) + \\\n",
    "            self.loss1(input=true_logit, target=torch.ones_like(true_logit))\n",
    "        return l1/2\n",
    "    \n",
    "from sklearn.metrics import classification\n",
    "def train_one_epoch(dl, net, loss_fn=None, optimizer=None, train_flag=True):\n",
    "    loss = SmoothenValue(beta=0.9)\n",
    "    if train_flag is True:\n",
    "        g_loss, d_loss = loss_fn\n",
    "        g_optimizer, d_optimizer = optimizer\n",
    "    #pbar = tqdm.tqdm_notebook(dl, leave=False)\n",
    "    y, y_ = [], []\n",
    "    for batch in dl:\n",
    "        batch_x, batch_y = batch[0].float().cuda(), batch[1].float().cuda()\n",
    "        batch_y_, batch_false_logit, batch_true_logit = net(batch_x, batch_y)\n",
    "        \n",
    "        if train_flag is True:\n",
    "            batch_g_loss = g_loss(batch_y_, batch_y, batch_false_logit)\n",
    "            g_optimizer.zero_grad(); batch_g_loss.backward(retain_graph=True); g_optimizer.step()\n",
    "            batch_d_loss = d_loss(batch_false_logit, batch_true_logit)\n",
    "            d_optimizer.zero_grad(); batch_d_loss.backward(); d_optimizer.step()\n",
    "            loss.add_value(batch_g_loss.item())\n",
    "            #pbar.set_description(desc=\"loss:%.4f\" % loss.smooth)\n",
    "        y.append(batch_y.cpu().numpy())\n",
    "        y_.append(batch_y_.detach().cpu().numpy())\n",
    "    y = np.concatenate(y, axis=0)\n",
    "    y_ = np.concatenate(y_, axis=0)\n",
    "    return y, y_, loss.mov_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, trn_dl, test_dl, net, loss_fn=None, optimizer=None):\n",
    "    metric = {'loss':[], 'acc':[], 'f1':[], 'micf1':[], 'macf1':[], 'ap':[]}\n",
    "    for i in range(epochs):\n",
    "        y1, y1_, trn_loss = train_one_epoch(trn_dl, net, loss_fn, optimizer)\n",
    "        trn_acc, trn_f1, trn_mic_f1, trn_mac_f1, trn_map = evaluate(y1, y1_, pivot=0.5)\n",
    "        y1, y1_, test_loss = train_one_epoch(test_dl, net, train_flag=False)\n",
    "        test_acc, test_f1, test_mic_f1, test_mac_f1, test_map = evaluate(y1, y1_, pivot=0.5)\n",
    "        #stdout.write(\"%d\\t\" % i)\n",
    "        #stdout.write(\"train: %.4f, %.4f, %.4f, %.4f, %.4f, %.4f\\t\" % (trn_loss, trn_acc, trn_f1, trn_mic_f1, trn_mac_f1, trn_map))\n",
    "        #stdout.write(\"test : %.4f, %.4f, %.4f, %.4f, %.4f, %.4f\\n\" % (test_loss, test_acc, test_f1, test_mic_f1, test_mac_f1, test_map))\n",
    "        metric['loss'].append(test_loss)\n",
    "        metric['acc'].append(test_acc)\n",
    "        metric['f1'].append(test_f1)\n",
    "        metric['micf1'].append(test_mic_f1)\n",
    "        metric['macf1'].append(test_mac_f1)\n",
    "        metric['ap'].append(test_map)\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_path = '/sdadata/zzq/dataset/MultiLabel/VOC2007/feature_ds_trainval.csv'\n",
    "trainval_df = pd.read_csv(trainval_path, index_col=0)\n",
    "test_path = '/sdadata/zzq/dataset/MultiLabel/VOC2007/feature_ds_test.csv'\n",
    "test_df = pd.read_csv(test_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyDataset(trainval_df)\n",
    "test_ds = MyDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=512, num_workers=0, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=512, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paramtune(alpha, epochs=10):\n",
    "    my_net = MyGAN().cuda()\n",
    "    d_optimizer = optim.Adam(params=my_net.discriminator.parameters(), lr=1e-3)\n",
    "    g_optimizer = optim.Adam(params=my_net.predictor.parameters(), lr=1e-3)\n",
    "    d_loss = MyDiscriminatorLoss().cuda()\n",
    "    g_loss = MyLoss(alpha).cuda()\n",
    "    res = train(epochs, train_dl, test_dl, my_net, loss_fn=[g_loss, d_loss], optimizer=[g_optimizer, d_optimizer])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb983af87aea4944bb49488a9ee8c6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:0.00000  mean-->acc:0.7259, micro-f1:0.7809, macro-f1:0.7512, map:0.8208 \tstd-->acc:0.0023, micro-f1:0.0019, macro-f1:0.0038, map:0.0027\n",
      "alpha:0.00010  mean-->acc:0.7257, micro-f1:0.7814, macro-f1:0.7516, map:0.8212 \tstd-->acc:0.0036, micro-f1:0.0019, macro-f1:0.0044, map:0.0032\n",
      "alpha:0.00100  mean-->acc:0.7255, micro-f1:0.7812, macro-f1:0.7525, map:0.8202 \tstd-->acc:0.0030, micro-f1:0.0018, macro-f1:0.0028, map:0.0026\n",
      "alpha:0.01000  mean-->acc:0.7271, micro-f1:0.7820, macro-f1:0.7526, map:0.8210 \tstd-->acc:0.0028, micro-f1:0.0017, macro-f1:0.0041, map:0.0034\n",
      "alpha:0.10000  mean-->acc:0.7282, micro-f1:0.7827, macro-f1:0.7544, map:0.8233 \tstd-->acc:0.0037, micro-f1:0.0025, macro-f1:0.0047, map:0.0016\n",
      "alpha:0.20000  mean-->acc:0.7264, micro-f1:0.7821, macro-f1:0.7541, map:0.8209 \tstd-->acc:0.0032, micro-f1:0.0021, macro-f1:0.0026, map:0.0025\n",
      "alpha:0.30000  mean-->acc:0.7273, micro-f1:0.7826, macro-f1:0.7539, map:0.8204 \tstd-->acc:0.0031, micro-f1:0.0021, macro-f1:0.0032, map:0.0022\n",
      "alpha:0.40000  mean-->acc:0.7259, micro-f1:0.7810, macro-f1:0.7526, map:0.8208 \tstd-->acc:0.0034, micro-f1:0.0024, macro-f1:0.0036, map:0.0021\n",
      "alpha:0.50000  mean-->acc:0.7277, micro-f1:0.7823, macro-f1:0.7526, map:0.8218 \tstd-->acc:0.0028, micro-f1:0.0018, macro-f1:0.0048, map:0.0020\n",
      "alpha:0.60000  mean-->acc:0.7253, micro-f1:0.7813, macro-f1:0.7525, map:0.8211 \tstd-->acc:0.0031, micro-f1:0.0026, macro-f1:0.0042, map:0.0024\n",
      "alpha:0.70000  mean-->acc:0.7262, micro-f1:0.7818, macro-f1:0.7527, map:0.8217 \tstd-->acc:0.0040, micro-f1:0.0032, macro-f1:0.0051, map:0.0026\n",
      "alpha:0.80000  mean-->acc:0.7272, micro-f1:0.7818, macro-f1:0.7533, map:0.8215 \tstd-->acc:0.0025, micro-f1:0.0021, macro-f1:0.0025, map:0.0024\n",
      "alpha:0.90000  mean-->acc:0.7267, micro-f1:0.7814, macro-f1:0.7529, map:0.8219 \tstd-->acc:0.0018, micro-f1:0.0015, macro-f1:0.0030, map:0.0027\n",
      "alpha:1.00000  mean-->acc:0.0720, micro-f1:0.1307, macro-f1:0.1189, map:0.0812 \tstd-->acc:0.0036, micro-f1:0.0059, macro-f1:0.0057, map:0.0044\n"
     ]
    }
   ],
   "source": [
    "alpha_set = [0.0, 0.0001,0.001,0.01,0.1,0.2,0.3, 0.4, 0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "times = 10\n",
    "for alpha in tqdm.tqdm_notebook(alpha_set):\n",
    "    res = {'loss':[], 'acc':[], 'f1':[], 'micf1':[], 'macf1':[], 'ap':[]}\n",
    "    for i in range(times):\n",
    "        tmp = paramtune(alpha)\n",
    "        res['loss'].append(np.array(tmp['loss']).min())\n",
    "        res['acc'].append(np.array(tmp['acc']).max())\n",
    "        res['f1'].append(np.array(tmp['f1']).max())\n",
    "        res['micf1'].append(np.array(tmp['micf1']).max())\n",
    "        res['macf1'].append(np.array(tmp['macf1']).max())\n",
    "        res['ap'].append(np.array(tmp['ap']).max())\n",
    "    stdout.write(\"alpha:%.5f  \" % alpha)\n",
    "    stdout.write(\"mean-->\")\n",
    "    #stdout.write(\"loss:%.4f, \" % np.array(res['loss']).mean())\n",
    "    stdout.write(\"acc:%.4f, \" % np.array(res['acc']).mean())\n",
    "    stdout.write(\"micro-f1:%.4f, \" % np.array(res['micf1']).mean())\n",
    "    stdout.write(\"macro-f1:%.4f, \" % np.array(res['macf1']).mean())\n",
    "    stdout.write(\"map:%.4f \\t\" % np.array(res['ap']).mean())\n",
    "    \n",
    "    stdout.write(\"std-->\")\n",
    "    #stdout.write(\"loss:%.4f, \" % np.array(res['loss']).std())\n",
    "    stdout.write(\"acc:%.4f, \" % np.array(res['acc']).std())\n",
    "    stdout.write(\"micro-f1:%.4f, \" % np.array(res['micf1']).std())\n",
    "    stdout.write(\"macro-f1:%.4f, \" % np.array(res['macf1']).std())\n",
    "    stdout.write(\"map:%.4f\\n\" % np.array(res['ap']).std())\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
